\chapter{Social network boosted search ranking}

Charles Explorer is an academic search engine, which allows the user to search for publications, classes, people and study programmes in the academic domain.

The following sections, we look more into the search engine part of the application, explore the possible issues with the search results ranking, 
and experiment with utilizing the social network data for the search results ranking improvement.

\section{Full-text search}

Full-text search is nowadays an essential part of any information retrieval system. 
Many search engines - including Apache Solr in Charles Explorer - implement the full text search by utilizing the TF-IDF algorithm.
This is a simple and efficient way to rank the search results based on the relevance of the documents to the search query.

The TF-IDF algorithm is based on the term frequency and inverse document frequency of the terms in the documents - a \textit{document} is in general unstructured free text content.
Some entities in our academic search engine map well to this notion of a \textit{document} - e.g. a \texttt{publication} or \texttt{class} both have inherent textual content (titles, abstracts or syllabi).
Unfortunately, this does not hold for all the entities in the system.

As an example, a \texttt{person} entity usually does not have any explicit textual content associated with it. 
When searching for a person interested in a particular topic, the search engine has to rely on the textual content of the publications, classes, 
or other entities associated with the person, i.e. traverse - at least implicitly - the knowledge graph (or the social network of the person).

A simple solution to this problem would be to represent every person as a document, concatenating all the textual content of the entities associated with the person.
This can be further refined by assigning different weights - e.g. to the different types of entities (a \texttt{class} might be more important than a \texttt{publication}), 
or different concatenated parts of the documents (e.g. the publication title and class name are more important than the abstracts and syllabi).

Regarding adding new entities related to a given person, the concatenation also serves us well - we can simply append the new entity's content to the person's document and reindex it.

However, this approach also has several drawbacks. First of all, assuming we're building a general academic search engine allowing for search in publications, classes and people, we would be indexing the same content multiple times.
This is not only inefficient in terms of storage, but also prone to update errors - there is multiple copies of the same content, which have to be updated separately.
The second issue arises from the concatenation - if any of the person's associated entities changes, the whole document has to be reindexed.
However, this might be less of a problem than the first issue, as it's not too common for academic records to get updated or removed - at least in comparison to the number of new records being added.

\subsection{Result ranking in the naive case} \label{search-ranking-issues}

Result ranking in information retrieval refers to the ordering of the search results when presented to the end user. This is often based on the relevance of the documents to the current search query.
The relevance-based ranking is often enough for the basic use case - the user is presented with the most relevant documents first, and can further explore the less relevant ones if needed.

While it might seem a bit superficial, the ranking is in fact still part of the information retrieval process. 
As multiple studies show \footnote{https://link.springer.com/article/10.1007/s11151-014-9435-y}\footnote{https://link.springer.com/article/10.1007/s10791-010-9150-8}, 
the ranking of the search results positively correlates with the click-through rate of the results 
- likely because of the typical top-left to bottom-right reading pattern of the users.
This can be further affected by other, more technical factors - such as the need for an additional user action like scroll or pagination to see the results further down the list.

%% https://link.springer.com/article/10.1007/s10791-010-9150-8 - Re-ranking search results using an additional retrieved list

Considering a simple tf-idf based search engine, the ranking of the search results is based on the relevance of the documents to the search query.
This is directly related to the term frequency of the query in the document - for a fixed query and a given document collection, we can forget about the inverse document frequency, as it's constant over all the documents.
Ranking the documents solely based on the term frequency might however lead to unfavourable results - especially in the case of a proxy-representation of a given entity.


\begin{figure}[ht!]
    \captionsetup{width=.9\linewidth}
    \includegraphics[width=0.8\textwidth]{../img/bob-alice-soc-netw.png}
    \centering
    \caption{Simple representation of the social network of Alice and Bob.}
\end{figure}

Let us explore the issues on an example where we represent people as documents, concatenating the textual content of the entities associated with them.
Consider two academic researchers in our system - \textit{prof. Alice} and \textit{Bc. Bob}. 
Bob is a student of Alice, and has published several papers on \textit{Information retrieval} with her. Aside from those, Bob has not published any other papers.
On the other hand, Alice has published a lot of papers on various topics - related to IR, but also to other similar fields. 
See a simple representation of their social network above.

Note that aside from the common publications, Bob has no other entities associated with him, while Alice has other publications with other co-authors.

\begin{figure}[ht!]
    \captionsetup{width=.9\linewidth}
    \includegraphics[width=0.8\textwidth]{../img/bob-alice-representations.png}
    \centering
    \caption{Concatenating representations of Alice and Bob as documents.}
\end{figure}

We see that the document of Bob is shorter than the one of Alice, because he has less associated entities. We can also notice that the Alice's document fully contains the Bob's document.
    
The colored terms represent the current search query - if Alice only published papers on the topic of the query with Bob, the term frequency of the 
query in Bob's document would be strictly higher, because of the shorter length.
In case Alice also publishes on the topic with other co-authors, it gets harder to reason about which of the term frequencies is higher.

The relation between the tf-idf scores of the documents of Alice and Bob can be expressed as follows:
\begin{subequations}
\begin{align}
\text{tf}(q, d_{\text{Bob}}) \times \text{idf}(q, D) &\stackrel{?}{>} \text{tf}(q, d_{\text{Alice}}) \times \text{idf}(q, D) \\
\text{tf}(q, d_{\text{Bob}}) &\stackrel{?}{>} \text{tf}(q, d_{\text{Alice}})
\end{align}
\end{subequations}

Normalized term frequency $\text{tf}_{q,d}$ of a term $q$ in a document $d$ is defined as the number of times the term $q$ appears in the document $d$ (denoted by $f_{q,d}$),
divided by the total number of terms in the document $d$ (the document's length).

\begin{subequations}
\begin{align}
\frac{f_{q, d_{\text{Bob}}}}{\text{len}(d_{\text{Bob}})} &\stackrel{?}{>} \frac{f_{q, d_{\text{Alice}}}}{\text{len}(d_{\text{Alice}})} \Biggm/ \;^{-1} \\
\frac{\text{len}(d_{\text{Bob}})}{f_{q, d_{\text{Bob}}}} &\stackrel{?}{<} \frac{\text{len}(d_{\text{Alice}})}{f_{q, d_{\text{Alice}}}} \Biggm/ \textit{assuming that } \mathit{f_{q, d_{\text{Bob}}} = f_{q, d_{\text{Alice}}}} \label{eq:51assumption}\\
\text{len}(d_{\text{Bob}}) \;&<\;\text{len}(d_{\text{Alice}})
\end{align}
\end{subequations}

Since we have assumed that Alice's document is a superset of Bob's document, the result inequality holds - and therefore the tf-idf score of Bob's document is higher than the one of Alice's document.

Note that the assumption in the equation \ref{eq:51assumption} is only true for the cases when Alice does not publish any other papers on the topic of the query with other co-authors.

% Because of this, the document representation of Bob will be short, compared to the one of Alice.
% Assuming the search query is \textit{Information Retrieval} - and there are no other publications or classes related to this topic in the system - 
% the search engine will consider Bob as the more relevant person for this topic. This is becuase the term frequency of the query in Bob's document is higher than in Alice's document - solely because of the length.

In either way, this approach most likely will not provide the desired outcome - while Bob has published papers only on the given topic - and therefore has higher tf-idf relevance to the query,
it is likely that Alice is the more relevant person for the searching user, since she is more experienced in the topic.

\section{Re-ranking}

In the following sections, we go over different existing re-ranking strategies utilized in other systems and explore the possibilities of using the social network data for the re-ranking in Charles Explorer.

\subsection{Existing re-ranking strategies}

In the current (2024) commercial search engines, there are often multiple re-ranking strategies available to the users.
Unfortunately, only a small portion of the engines actually discloses the details of the re-ranking algorithms used - perhaps to protect the intellectual property and the competitive advantage.

The common grounds for all those systems is the two-stage search pipeline - in the first stage, a traditional search engine is used to retrieve the initial set of results.
Then, these results are re-ranked using a different algorithm. 

While it might seem redundant, this approach allows the second algorithm to focus only the more relevant results, and be perhaps more computationally expensive.
Unfortunately, this also means that the re-ranking algorithm is not able to affect the initial search results - it can only change the order of the results.
It also brings in the issue of pagination - if the user has to go through multiple pages of the search results, the re-ranking might not be as effective,
as it only affects the current page of the results.

The re-ranking algorithms can be based on various factors - the user's previous interactions with the system, total popularity (click-through rate) of the items,
diversity of the results %link to the MMR papers https://www.google.com/search?client=firefox-b-lm&q=Maximal+Marginal+Relevance 
or NLP analysis of the documents. Especially the last one seems to be gaining traction in the recent years, as the applications of LLM models are becoming more widespread.

\subsection{Graph data-based reranking}

As we discussed in \ref{search-ranking-issues}
, the traditional tf-idf based search engine might give unsatisfactory results in the case 
of a proxy-representation of a given entity.
We might try to address this issue by utilizing the social network data for the re-ranking of the search results.

As mentioned in the previous subsection, many commercial search engines use a two-step search pipeline - the first step retrieves the initial set of results,
and the second step re-ranks them using a different algorithm. This sounds like a good fit for our problem - we can use the traditional tf-idf based search engine 
to retrieve the initial set of results, and then re-rank them using the social network data. 

Only using the graph data for the search has several drawback described in the next section (\ref{graph-based-search}).

When it comes to reranking the existing search results, we can utilize the social network data in several ways.

\begin{itemize}
    \item \textbf{Global node statistics}: As illustrated on the example of Alice and Bob above, 
    we can use the global statistics for the nodes in the social network to re-rank the search results. 
    These include - but are not limited to - the node degree (number of connections), centrality measures, or the number of common neighbours.

    Such statistics can be precomputed for every node in the social network before the search and stay constant over all the search queries.

    \item \textbf{Query relevant node statistics}: Unlike the global node statistics, query-relevant node statisics are computed only for the nodes in the search results.
    
    These can perhaps define the node's relevance better (e.g. the number of query-relevant publications connected to a given person), but are often more computationally expensive to compute.
    Given the variability of the queries, it is also harder (or impossible) to precompute these statistics.
\end{itemize}

%% todo structure
\subsection{Graph data-based search} \label{graph-based-search}

It would also be possible to use the social network data to entirely replace the concatenated person representations in the search engine - we could 
only search through the ``first-order entities'' (i.e. classes and publications) and then query the social network graph for the people associated 
with the results of the search. 

This approach has several drawbacks - aside from the questionable performance and scalability / developer effort, it also might not be as effective as the concatenated representation.

With the concatenated approach, we have to retrieve only \textasciitilde{} 30 results (people) for every page of the search results. 
Those are also already ranked by the relevance to the query - even though this might not be perfect, the tf-idf based ranking is still a good approximation.

With the proposed graph-based approach, we would have to retrieve many more publications and classes on each search - note that we are no longer 
looking for the most-relevant class or publication, but for the most-relevant person - which is rather an aggregation of the relevance of the associated entities.
This might lead to a lot of unnecessary data retrieval and processing and become the bottleneck for the possible applications built on such data.


\section{Benchmarks}

To determine the performance of our proposed re-ranking solution, we establish benchmarks to compare the results of the traditional tf-idf based search engine with the social network enhanced search engine.

The common denominator for many of the ranking measures - like discounted cumulative gain or mean reciprocal rank - is the \textit{user interaction}.
The user is presented with the search results and picks the most relevant one or scores the results based on the relevance to the query.

Unfortunately, this is not applicable to our case - while we are tracking the user interactions in the Charles Explorer web appplication, the amount of collected data is far too low to be statistically significant.

\begin{mybox}{}
    The following section is described in greater detail in my blog post \textit{Benchmarking Charles Explorer search result rankings}\footnote{\url{https://jindrich.bar/edu/thesis-blog/ranking-benchmarks/}}.
    
    The blog post contains the detailed description of the process of collecting the search queries for the benchmarking of the search results ranking in Charles Explorer.

    It also features reproducible code snippets used for the data collection and processing.
\end{mybox}

To establish a gold standard for the search results relevance, we do not have to rely solely on human interactions.

Elsevier Scopus\footnote{https://www.elsevier.com/products/scopus} is a large academic database, which provides a search engine for the academic publications.
Aside from the web application, it also provides a REST API for consuming the data programmatically.

We can use the Scopus API to retrieve ranked lists of publications for a given query, and then use the ranking of the publications as the source of the ``global relevance'' for the search query in the benchmark.
Simply put, by comparing the (ranking of the) search results in our search engine to the results of the Scopus search engine, we can determine the relevance of the search results.

We are expecting the search results of the Scopus search engine to be more precise and relevant than the ones of the Charles Explorer search engine -
Scopus is a commercial product with a large team of developers and researchers, while Charles Explorer is a small academic project.
The data available to scopus also contain details about the citations of the publications and author profiles, which can be used to further improve the search results ranking.

\subsection{Sampling the search query set}

As the first step, we need to sample the \textit{search query set} for the benchmark. 
Since we want to rule out possible biases - or at least mitigate their impact - we need a large and diverse enough set of queries to compare the search engines on.

Generating these manually would be time-consuming and error-prone. 
Therefore, to solve this issue, we can use a \textit{wordnet} - a lexical language database of English.
We can use it to generate a large set of diverse queries, perhaps less biased than a manually generated set.

We start by selecting a set of \textit{seed words} - in our experimental case, those were the words \textit{``field of study''} and \textit{``medicine''}.
Then, we traverse the wordnet to recursively find the hyponyms of those seed words, up to a certain depth.

Running this process for the seed words \textit{``field of study''} and \textit{``medicine''} with the depth of 4, we get a set of \texttt{915} search queries.

While this approach gives us a sizable set of queries, we have no guarantee of the quality of the queries - they might be too general or too specific, or not relevant to the academic domain at all.
One of our goals was also to ensure the fairness of the query set - this is not guaranteed by the wordnet traversal either, as the queries might be too similar to each other (or target the similar topics in the publications).

\subsubsection{Ensuring the query set fairness}

While \textit{fairness} can be a largely subjective measure, we can let the available data guide us in this case.
For the \textit{academic publications}, we have their \textit{titles}, \textit{abstracts}, \textit{keywords}, \textit{faculty affiliations} and \textit{authors} avaialble in our system.

Since titles, abstracts and keywords are free text fields, we can omit them from our analysis - the preprocessing of the text data is a complex task on its own.
Given the nature of our experiment - i.e. measuring the impact of using the social network data for the search results ranking - we have to leave the authorship information out as well.

This process leaves us with the faculty affiliations.

Charles University has \textit{17} faculties, each with a different focus and research areas.
Each publication in our data is attributed to exactly one faculty.
This allows us to use the faculty affiliations as a proxy for the fairness of the search queries.

\textbf{Kullback-Leibler divergence}

As the fairness measure, we can compare the distribution of the faculty affiliations in the search query results 
to the distribution of the faculty affiliations of all the publications in the system.

The standard way of comparing probability distributions is the \textit{Kullback-Leibler divergence} - a measure of how one probability distribution diverges from a second, expected probability distribution.

For discrete probability distributions $P$ and $Q$ defined on the same sample space $\Omega$, the Kullback-Leibler divergence from $Q$ to $P$ is defined as

$$
D_{KL}(P||Q) = \sum_{\omega \in \Omega} P(\omega) \log \frac{P(\omega)}{Q(\omega)}
$$

The KL-divergence is always non-negative, and is zero if and only if $P$ and $Q$ are the same distribution.

With the measure of the fairness of the search query set established, we can now proceed to the benchmarking of the search results ranking in Charles Explorer.

By sampling up to 30 results for each search query from the Charles Explorer search engine, we acquire the faculty distribution for the entire search query set ($N = 915$).
We can then compare this distribution to the distribution of the faculty affiliations of all the publications in the system.

\begin{figure}[ht!]
    \captionsetup{width=.9\linewidth}
    \includegraphics[width=0.8\textwidth]{../img/all-queries-vs-totals.png}
    \centering
    \caption{Comparing the faculty distribution of the search query results \textit{(orange, right)} to the distribution of all the publications in the system \textit{(blue, left)}.}
\end{figure}

The Kullback-Leibler divergence of the faculty distribution of the \textbf{search query results} from the distribution of \textbf{all the publications} in the system is approximately \texttt{0.0471}.

\textbf{Optimizing the KL-divergence}

With the defined measure of the fairness of the search query set, we can now try to optimize it.
In our case, optimizing the KL-divergence means \textit{finding a subset} of search queries which would minimize the divergence of the faculty distribution of the search query results from the distribution of all the publications in the system.

Unfortunately, this poses serious challenges. 
Finding a subset with an optimal aggregate property is a well-known NP-hard problem - often referred to as the \textit{0-1 knapsack problem} or the \textit{subset sum problem}.
Even worse, we cannot simply reuse some of the existing algorithms for these problems, as those rely on the distributivity and associativity of the sum operation. 
This is however not the case for the KL divergence.

Similarly to the sum of the item values (in the Knapsack problem), the KL divergence is evaluated on the entire set, but unlike the sum, the items themselves do not have any “value” - and their contribution to the KL divergence changes depending on the other items in the set. 
This leaves us with a limited choice of algorithms to solve the problem. 
Because of the complexity of the problem and its smallish role in this work, we use a simple random search. 

This approach works in two steps:

\begin{enumerate}
    \item Repeatedly sample a random subset of size $k$ of the search queries, keeping track of the subset with the lowest KL divergence.
After a fixed number of iterations, take the subset with the lowest KL divergence.
    \item From the $k$-sized subset, repeatedly remove the search query with the highest contribution to the divergence.
Stop when the KL-divergence stops decreasing, i.e. we cannot remove any more search queries to decrease the divergence, or we've reached the minimum subset size $l$.
\end{enumerate}

This is a simple and computationally cheap approach, but it might not always find the optimal solution.
Furthermore, since the KL-divergence is a non-convex function, the second step of the algorithm might get stuck in a suboptimal solution.
While this could be mitigated using a more complex optimization algorithm (e.g. simulated annealing etc.), experimental results show that the simple random search is sufficient for our purposes.

\begin{figure}[ht!]
    \captionsetup{width=.9\linewidth}
    \includegraphics[width=0.8\textwidth]{../img/all-queries-vs-totals-corrected.png}
    \centering
    \caption{Comparing the faculty distribution of the search query results \textit{(orange, right)} to the distribution of all the publications in the system \textit{(blue, left)} \textit{after running the optimization algorithm}.}
\end{figure}

Running the optimization algorithm with $n = 10000$ initial random samples of size $k = 300$ on the search query set, we arrive at a set of $174$ queries with the search result KL-divergence of \texttt{0.00317}. 
This is a significant improvement over the original divergence of \texttt{0.0471} and we will consider this a fairer subset of the search queries for the benchmarking.