\chapter{Social network boosted search ranking}

In the previous chapter, we explored the process of transforming the relational academic data 
into a true graph representation, including using graph metrics for missing data inference.

In this chapter, we focus on the \textit{search engine} part of the Charles Explorer application.
Using the graph data representation from the previous chapter, we explore the possible issues with the classic 
document-based search engine and experiment with the social network data for the search results re-ranking.

\section{Full-text search}

Full-text search is nowadays an essential part of any information retrieval system. 
Many search engines - including Apache Solr in Charles Explorer - implement the full text search by utilizing the TF-IDF algorithm or similar.
This is a simple and efficient way to rank the search results based on the relevance of the documents to the search query.

The TF-IDF algorithm is based on the term frequency and inverse document frequency of the terms in the documents - a \textit{document} is in general unstructured free text content.
Some entities in our academic search engine map well to this notion of a \textit{document} - e.g. a \texttt{publication} or \texttt{class} both have inherent textual content (titles, abstracts or class syllabi).
Unfortunately, this does not hold for all the entities in the system.

As an example, a \texttt{person} entity usually does not have any explicit textual content associated with it. 
When searching for a person interested in a particular topic, the search engine has to rely on the textual content of the publications, classes, 
or other entities associated with the person, i.e. traverse - at least implicitly - the knowledge graph (or the social network of the person).

A simple solution to this problem would be to represent every person as a document, concatenating all the textual content of the entities associated with the person.
This can be further refined by assigning different weights - e.g. to the different types of entities (a \texttt{class} might be more important than a \texttt{publication}), 
or different concatenated parts of the documents (e.g. the publication title and class name are more important than the abstracts and syllabi).

Regarding adding new entities related to a given person, the concatenation also serves us well - we simply append the new entity's content to the person's document and reindex it.

However, this approach also has several drawbacks. First of all, assuming we're building a general academic search engine allowing for search in publications, classes and people, we would be indexing the same content multiple times.
This is not only inefficient in terms of storage, but also prone to update errors - there is multiple copies of the same content, which have to be updated separately.
The second issue arises from the concatenation - if any of the person's associated entities changes, the whole document has to be reindexed.
However, this might be less of a problem than the first issue, as it's not too common for academic records to get updated or removed - at least in comparison to the number of new records being added.

\subsection{TD-IDF ranking issues} \label{search-ranking-issues}

Result ranking in information retrieval refers to the ordering of the search results when presented to the end user. This is often based on the relevance of the documents to the current search query.
The relevance-based ranking is often enough for the basic use case - the user is presented with the most relevant documents first, and can further explore the less relevant ones if needed.

While it might seem a bit superficial, the ranking is in fact still part of the information retrieval process. 
\cite{ranking-user-choice} showed that
the ranking of the search results positively correlates with the click-through rate of the results 
- likely because of the typical top-left to bottom-right reading pattern of the users.
This can be further affected by other, more technical factors - such as the need for an additional user action like scroll or pagination to see the results further down the list.

%% https://link.springer.com/article/10.1007/s10791-010-9150-8 - Re-ranking search results using an additional retrieved list

Considering a simple tf-idf based search engine, the ranking of the search results is based on the relevance of the documents to the search query.
This is directly related to the term frequency of the query in the document - for a fixed query and document collection, we can forget about the inverse document frequency, as it's constant over all the documents.
Ranking the documents solely based on the term frequency might however lead to unfavourable results - especially in the case of a proxy-representation of a given entity.


\begin{figure}[ht!]
    \captionsetup{width=.9\linewidth}
    \includegraphics[width=0.8\textwidth]{../img/bob-alice-soc-netw.png}
    \centering
    \caption{Simple representation of the social network of Alice and Bob.}
\end{figure}

Let us explore the issues on an example where we represent people as documents, concatenating the textual content of the entities associated with them.
Consider two academic researchers in our system - \textit{prof. Alice} and \textit{Bc. Bob}. 
Bob is a student of Alice, and has published several papers on \textit{Information retrieval} with her. Aside from those, Bob has not published any other papers.
On the other hand, Alice has published a lot of papers on various topics - related to IR, but also to other similar fields. 
See a simple representation of their social network above.

Note that aside from the common publications, Bob has no other entities associated with him, while Alice has other publications with other co-authors.

\begin{figure}[ht!]
    \captionsetup{width=.9\linewidth}
    \includegraphics[width=0.8\textwidth]{../img/bob-alice-representations.png}
    \centering
    \caption{Concatenating representations of Alice and Bob as documents.}
\end{figure}

We see that the document of Bob is shorter than the one of Alice, because he has less associated entities. We also notice that the Alice's document fully contains the Bob's document.
    
The colored terms represent the current search query - if Alice only published papers on the topic of the query with Bob, the term frequency of the 
query in Bob's document would be strictly higher, because of the shorter document length.
In case Alice also publishes on the topic with other co-authors, it gets harder to reason about which of the term frequencies is higher.

The relation between the tf-idf scores of the documents of Alice and Bob can be expressed as follows:
\begin{subequations}
\begin{align}
\text{tf}(q, d_{\text{Bob}}) \times \text{idf}(q, D) &\stackrel{?}{>} \text{tf}(q, d_{\text{Alice}}) \times \text{idf}(q, D) \\
\text{tf}(q, d_{\text{Bob}}) &\stackrel{?}{>} \text{tf}(q, d_{\text{Alice}})
\end{align}
\end{subequations}

Normalized term frequency $\text{tf}_{q,d}$ of a term $q$ in a document $d$ is defined as the number of times the term $q$ appears in the document $d$ (denoted by $f_{q,d}$),
divided by the total number of terms in the document $d$ (the document's length).

\begin{subequations}
\begin{align}
\frac{f_{q, d_{\text{Bob}}}}{\text{len}(d_{\text{Bob}})} &\stackrel{?}{>} \frac{f_{q, d_{\text{Alice}}}}{\text{len}(d_{\text{Alice}})} \Biggm/ \;^{-1} \\
\frac{\text{len}(d_{\text{Bob}})}{f_{q, d_{\text{Bob}}}} &\stackrel{?}{<} \frac{\text{len}(d_{\text{Alice}})}{f_{q, d_{\text{Alice}}}} \Biggm/ \textit{assuming that } \mathit{f_{q, d_{\text{Bob}}} = f_{q, d_{\text{Alice}}}} \label{eq:51assumption}\\
\text{len}(d_{\text{Bob}}) \;&<\;\text{len}(d_{\text{Alice}})
\end{align}
\end{subequations}

Since we have assumed that Alice's document is a superset of Bob's document, the result inequality holds - and therefore the tf-idf score of Bob's document is higher than the one of Alice's document.

Note that the assumption in the equation \ref{eq:51assumption} is only true for the cases when Alice does not publish any other papers on the topic of the query with other co-authors.

% Because of this, the document representation of Bob will be short, compared to the one of Alice.
% Assuming the search query is \textit{Information Retrieval} - and there are no other publications or classes related to this topic in the system - 
% the search engine will consider Bob as the more relevant person for this topic. This is becuase the term frequency of the query in Bob's document is higher than in Alice's document - solely because of the length.

In either way, this approach most likely will not provide the desired outcome - while Bob has published papers only on the given topic - and therefore has higher tf-idf relevance to the query,
it is likely that Alice is the more relevant person for the searching user, since she is more experienced in the topic.

\section{Re-ranking}

Knowing the theoretical issues of the current Charles Explorer search engine implementation, we try to address them by proposing a system for re-ranking the search results.
In the following sections, we go over different existing re-ranking strategies utilized in other systems and explore the possibilities of using the social network data 
for the re-ranking in Charles Explorer.

Even though we have only talked about person entities in the previous sections, we will conduct the all the experiments
on the ranking of the search results for \textbf{publication} records.
This is because the publications are uniquely identifiable (unlike people - see \ref{sec:inferring-missing-identities} for more details) 
and they have implicit textual content associated with them - the title, abstract and keywords.

Note that the issues from \ref{search-ranking-issues} still apply for the publications - \cite{aseo} mention different approaches to \ac{ASEO}.
By overusing certain keywords in the publication titles and abstracts, the authors can artificially increase the relevance of their publications to the search queries.
Reranking the publications by a less volatile and less falsifiable metric - like the social network data - might help to mitigate this issue.

\subsection{Existing re-ranking strategies}

In the current (2024) commercial search engines, there are often multiple re-ranking strategies available to the users.
Unfortunately, only a small portion of the engines actually discloses the details of the re-ranking algorithms used - perhaps to protect the intellectual property and the competitive advantage.

To have a look at the existing re-ranking strategies, we examine the related academic literature on the topic.

\cite{wikipedia-rerank} propose a reranking algorithm for the Wikipedia search engine, ranking the search results based on a current wikipedia page and its linkage (paths in the web graph) to the search results.
\cite{passage-graph} propose a reranking algorithm for document search using metrics on a \textit{passage graph} - a graph representation of the documents and the passages within them
with the edges representing the similarity between the passages.

Famously, \cite{pagerank} proposed the PageRank algorithm for the Google search engine.
The PageRank algorithm itself does not rerank the search results - it is used to produce scoring of the web pages based on the link structure of the web.
The ranking of the search results is then based on multiple factors, including the PageRank score of the pages.

Note that reranking is not only applicable in the context of search engines - recommender systems use similar strategies of reordering the presented items based on the user's preferences or other factors.
\cite{personalized-reranking} propose a personalized reranking algorithm for the recommendation systems, which takes into account the user's preferences and the item's popularity.
See that unlike in the search engines, the reranking in the recommendation systems is based on the user's interactions with the system, rather on the ``global'' statistics of the items.

\subsection{Algorithm}\label{reranking-algorithm}

In the previous subsection, we have listed several exisiting reranking strategies.
The common base for all these systems is the \textbf{two-stage search pipeline}. 
In the first stage, a traditional search engine (e.g. TF-IDF based) is used to retrieve the initial set of results. 
Then, these results are re-ranked using a different algorithm. 

While this might seem redundant, this approach allows the second algorithm to focus only the more relevant results, 
and be perhaps more computationally expensive.
Unfortunately, this also means that the reranking algorithm is not able to affect the initial search results; 
it can only change the order of the results. It also brings in the issue of pagination - if the user has to go 
through multiple pages of the search results, the reranking might not be as effective, as it only affects the current page of the results.

Based on this framework, we propose a reranking algorithm for the Charles Explorer search engine:

\textit{For a query $q$, a set of all publications $D$ and a social network $G$ of all the people and publications in the system, 
we propose the following reranking algorithm:}

\begin{enumerate}
    \item Retrieve the initial list $R_q$ of search results relevant to $q$, $R_q \subset D$ using the traditional search engine.
    $R_q$ is an ordered list of publications, ranked by the relevance to the query.
    \item For every publication $p \in R_q$, get the auxiliary relevance score $s(p)$ \textbf{based on the social network data}.
    \item Construct a new list $\text{Rank}_q$ with items from $R_q$ and ranking based both on the original relevance scores and the auxiliary relevance scores.
\end{enumerate}

We notice that this algorithm definition is quite abstract - and aside from the mention of the social network data, 
it does not differ from the general two-stage reranking algorithm definition.

However, the third step of the algorithm is not trivial either. The balance of the original relevance scores and the auxiliary relevance scores
in the final ranking is a task of \textit{multiobjective optimization} and might be hard to solve in a general case.
Later on, we try to experimentally evaluate the performance of different ranking-combination strategies.

In the following sections, we explore different social network metrics that can be used for auxiliary relevance score $s(p)$ calculation.

\subsection{Social network metrics for reranking}\label{social-network-metrics}

The academic social network graph is often large and sparse, as academic systems often accumulate many records over their lifetime.
This poses a challenge for the social network metrics calculation - the computational complexity of any global graph metric grows 
with the number of nodes in the graph.

This combined with the need for the real-time response of the search engine makes the calculation of the global graph metrics infeasible.
In the following sections, we propose some of the measures defined on close neighbourhoods of the nodes in the graph,
the performance of which we will later evaluate in the experiments.

\subsubsection{Node degree}

The \textit{degree} of a node $v$ in a graph $G$ is the number of edges incident to $v$.
$$
\text{deg}(v) = |N(v)|
$$

In the case of the publications nodes, the node degree is the number of people that have collaborated on the publication.
The possible benefit of using this metric is rather clear - we expect that might be a correlation between the number of collaborators on a publication and the 
global relevance of a publication.

\subsubsection{Neighborhood-separating node cut size}

Similarly to the \textit{node degree}, we define the \textit{neighborhood-separating node cut size} measure.
For a given node $n$ and its neighbourhood $N(n)$, we define the \textit{neighborhood-separating node cut size} 
as the number of nodes (outside of $N(n)$) that have to be removed to separate $N(n)$ from the rest of the graph.

In the context of the academic social network, for a publication $p$, this counts the number of other publications the authors of $p$ have authored.
Note that in case the authors of $p$ have collaborated on some other publications, those still only contribute to the cut size once.

We can also calculate this measure as 
$$
\text{cut}(n) = \lvert N_2(n) \rvert - \lvert N_1(n) \rvert
$$

where $N_k(n)$ is the k-hop neighbourhood of the node $n$.

\subsubsection{Ego betweenness centrality}\label{ego-betweenness}

\textit{Betweenness centrality} of a node $v$ in a graph $G$ is the fraction of all shortest paths that pass through $v$.
$$
\text{betw}(v) = \sum_{s \neq v \neq t} \frac{\sigma_{st}(v)}{\sigma_{st}}
$$
where $\sigma_{st}$ is the number of shortest paths between nodes $s$ and $t$, and $\sigma_{st}(v)$ is the number of those paths that pass through $v$.

While this is usually calculated in the context of the entire graph, it is an useful measure for ego-networks too, as it can help us quantify the importance of a node in its local neighbourhood. 
\cite{egonetworkbetweenness} have shown that for real-life networks, the ego betweenness centrality often correlates with the actual global betweenness of a node in the graph.

In our data, the collaboration graph is bipartite - the nodes are either publications or people and there are no edges between the nodes of the same type.
This means that the ego betweenness centrality of a publication is in fact proportional to the number of people that have collaborated on the publication.

\subsubsection{2-hop betweenness centrality}
Similar to the \hyperref[ego-betweenness]{ego betweenness centrality}, we calculate the \textit{2-hop betweenness centrality} as the betweenness centrality of a node 
in a subgraph induced by the node and its 2-hop neighbourhood.

In our case of the bipartite collaboration graph, the \textit{2-hop betweenness centrality} of a publication is no longer proportional only to the number of people that have collaborated on the publication,
but also to the \textit{number of other publications that the people have collaborated on}.

Note that this concept can be further extended to the $k$-hop betweenness centrality, but the computational complexity of the centrality calculation grows exponentially with the $k$.
Materializing the induced subgraphs for the $k$-hop betweenness centrality calculation also poses a challenge in regard to the memory consumption. 

In our experiements, we only use the $1-$ and $2-hop$ neighborhoods for the betweenness centrality calculation, due to the fast growth of the computational complexity with larger $k$.

\subsubsection{Eigenvector centrality}
The \textit{eigenvector centrality} of a node $v_i$ in a graph $G$ is the sum of the centrality scores of the nodes that are connected to $v$.
$$
\text{eig}(v_i) = \frac{1}{\lambda} \sum_{v_j \in N(v)} \text{eig}(v_j)
$$

where $\lambda$ is a constant. Note that the definition can also be rewritten as 
$$
\text{eig}(v_i) = \frac{1}{\lambda} \sum_{v_j \in V} a_{v_i, v_j} \text{eig}(v_i)
$$

with $a_{vu}$ being the elements of the adjacency matrix of the graph $G$.

Denoting $\mathbf{eig(v)}$ as a vector of eigenvector centralities for all nodes in the graph, the equation can be rewritten as

$$
\lambda \; \mathbf{eig(v)} = A \; \mathbf{eig(v)}
$$

where $A$ is the adjacency matrix of the graph $G$. 
This shows the reasoning behind the name of the centrality measure - the centrality scores are the eigenvectors of the adjacency matrix.

Note that we are mentioning this measure only because of the following mentioned measure.
We do not calculate the eigenvector centrality in our experiments, as the computational complexity of the centrality calculation is far too high for the large graphs.

\subsubsection{Katz centrality}\label{def:katz-centrality}
\textit{Katz centrality} is a special case of the eigenvector centrality.

\cite{Katz1953} defines the \textit{Katz centrality} of a node $v_i$ in a graph $G$ as
$$
\text{katz}(v_i) = \sum_{k=1}^{\infty} \sum_{j=1}^n \alpha^k (A^k)_{ij}
$$
where $\alpha \in (0, 1/\lambda_{\text{max}})$ is a constant, and $\lambda_{\text{max}}$ is the largest eigenvalue of the adjacency matrix $A$.

The Katz centrality counts the number of paths between the central node and all other nodes in the graph.
For a path of length $k$ is additionally discounted by the \textit{attenuation} factor $\alpha^k$.

The above formula uses the fact that the power of the adjacency matrix $A^k$ counts the number of paths of length $k$ between nodes $i$ and $j$.

\begin{mybox}{}
As follows from the definitions, the betweenness centrality of a node $v$ is computationally expensive 
to calculate for large graphs, as it considers all pairs of nodes in the graph.

While e.g. \cite{brandes-faster-centrality} offers an algorithm that reduces the computational complexity of the betweenness centrality calculation,
the calculation still poses a significant performance bottleneck for large graphs.

\label{katz-faster}
On the other hand, the Katz centrality can be calculated more efficiently - since the definition introduces 
the attenuation factor $\alpha$, the importance of the nodes that are further away from the central node is discounted
and the centrality measure can be approximated by truncating the sum at a certain small value of $k$.
\end{mybox}

\subsection{Combining the metrics}\label{combining-metrics}

In the third step of our proposed \hyperref[reranking-algorithm]{reranking algorithm},
we want to combine the original relevance scores of the publications with the auxiliary 
relevance scores based on the social network metrics.

In our experiments, we compare two different strategies for combining the scores:

\begin{enumerate}
    \item \textbf{Linear combination} - the final relevance score of a publication is a linear combination of the original relevance score and the auxiliary relevance score. 
    
    This includes edge cases of 100\% weight on the original relevance score or the auxiliary relevance score.

    $$
    \text{final\_score}(p) = \alpha \cdot \text{original\_score}(p) + (1 - \alpha) \cdot \text{auxiliary\_score}(p)
    $$
    \item \textbf{Neural networks} - we train a neural network to predict the final relevance score of a publication based on the original relevance score and the auxiliary relevance score.
    
    Note that this approach is more complex and computationally expensive than the linear combination, but it might provide better results,
    as it allows for the non-linear combination of the scores.
\end{enumerate}
    
\section{Benchmarking setup}

To determine the performance of our proposed re-ranking solution, we establish benchmarks to compare the results of the traditional tf-idf based search engine with the social network enhanced search engine.

\subsection{Sourcing the golden data}

The common denominator for many of the ranking measures - like \ac{DCG} or \ac{MRR} - is the \textit{user interaction}.
The user is presented with the search results and picks the most relevant one or scores the results based on the relevance to the query.

Unfortunately, this is not applicable to our case - while we are tracking the user interactions in the Charles Explorer web appplication, the amount of collected data is far too low to be statistically significant.

To establish a gold standard for the search results relevance, we do not have to rely solely on human interactions.

Elsevier Scopus\footnote{https://www.elsevier.com/products/scopus} is a large academic database, which provides a search engine for the academic publications.
Aside from the web application, it also provides a REST API for consuming the data programmatically.

We use the Scopus API to retrieve ranked lists of publications for different queries, and then use the ranking of the publications as the source of the ``global relevance'' for the search query in the benchmark.
Simply put, by comparing the (ranking of the) search results in our search engine to the results of the Scopus search engine, we determine the relevance of the search results.

We are expecting the search results of the Scopus search engine to be more precise and relevant than the ones of the Charles Explorer search engine -
Scopus is a commercial product with a large team of developers and researchers, while Charles Explorer is a small academic project.
The data available to Scopus also contain details about the citations of the publications and author profiles, which can be used to further improve the search results ranking.

To partially mitigate the possible bias of the Scopus search engine, 
we also evaluate the search results of the graph-enhanced reranking against 
ranking based on the citation count. 

We can consider the citation count as a proxy for the global relevance of the publication - the more citations a publication has, the more relevant it is to the academic community.

\subsection[Sampling the search query set]{Sampling the search query set\textsuperscript{\href{https://jindrich.bar/edu/thesis-blog/ranking-benchmarks/}{[blog]}}}

As the first step, we need to sample the \textit{search query set} for the benchmark. 
Since we want to rule out possible biases - or at least mitigate their impact - we need a large and diverse enough set of queries to compare the search engines on.

Generating these manually would be time-consuming and error-prone. 
Therefore, to solve this issue, we use a \textit{wordnet} - a lexical language database of English.
We use it to generate a large set of diverse queries, perhaps less biased than a manually generated set.

We start by selecting a set of \textit{seed words} - in our experimental case, those were the words \textit{``field of study''} and \textit{``medicine''}.
Then, we traverse the wordnet to recursively find the hyponyms of those seed words, up to a certain depth.

Running this process for the seed words \textit{``field of study''} and \textit{``medicine''} with the depth of 4, we get a set of \texttt{915} search queries.

While this approach gives us a sizable set of queries, we have no guarantee of the quality of the queries - they might be too general or too specific, or not relevant to the academic domain at all.
One of our goals was also to ensure the fairness of the query set - this is not guaranteed by the wordnet traversal either, as the queries might be too similar to each other (or target the similar topics in the publications).

\subsubsection{Ensuring the query set fairness}

While \textit{fairness} is a largely subjective measure, we let the available data guide us in this case.
For the \textit{academic publications}, we have their \textit{titles}, \textit{abstracts}, \textit{keywords}, \textit{faculty affiliations} and \textit{authors} avaialble in our system.

Since titles, abstracts and keywords are free text fields, we omit them from our analysis - the preprocessing of the text data is a complex task on its own.
Given the nature of our experiment - i.e. measuring the impact of using the social network data for the search results ranking - we have to leave the authorship information out as well.

This process leaves us with the faculty affiliations.

Charles University has \textit{17} faculties, each with a different focus and research areas.
Each publication in our data is attributed to exactly one faculty.
This allows us to use the faculty affiliations as a proxy for the fairness of the search queries.

\textbf{Kullback-Leibler divergence}

As the fairness measure, we compare the distribution of the faculty affiliations in the search query results 
to the distribution of the faculty affiliations of all the publications in the system.

The standard way of comparing probability distributions is the \textit{Kullback-Leibler divergence} - a measure of how one probability distribution diverges from a second, expected probability distribution.

For discrete probability distributions $P$ and $Q$ defined on the same sample space $\Omega$, the Kullback-Leibler divergence from $Q$ to $P$ is defined as

$$
D_{KL}(P||Q) = \sum_{\omega \in \Omega} P(\omega) \log \frac{P(\omega)}{Q(\omega)}
$$

The KL-divergence is always non-negative, and is zero if and only if $P$ and $Q$ are the same distribution.

With the measure of the fairness of the search query set established, we now proceed to the benchmarking of the search results ranking in Charles Explorer.

By sampling up to 30 results for each search query from the Charles Explorer search engine, we acquire the faculty distribution for the entire search query set ($N = 915$).
We then compare this distribution to the distribution of the faculty affiliations of all the publications in the system.

\begin{figure}[ht!]
    \captionsetup{width=.9\linewidth}
    \includegraphics[width=0.8\textwidth]{../img/all-queries-vs-totals.png}
    \centering
    \caption{Comparing the faculty distribution of the search query results \textit{(orange, right)} to the distribution of all the publications in the system \textit{(blue, left)}.}
\end{figure}

The Kullback-Leibler divergence of the faculty distribution of the \textbf{search query results} from the distribution of \textbf{all the publications} in the system is approximately \texttt{0.0471}.

\textbf{Optimizing the KL-divergence}

With the defined measure of the fairness of the search query set, we now try to optimize it.
In our case, optimizing the KL-divergence means \textit{finding a subset} of search queries which would minimize the divergence of the faculty distribution of the search query results from the distribution of all the publications in the system.

Unfortunately, this poses serious challenges. 
Finding a subset with an optimal aggregate property is a well-known NP-hard problem - often referred to as the \textit{0-1 knapsack problem} or the \textit{subset sum problem}.
Even worse, we cannot simply reuse some of the existing algorithms for these problems, as those rely on the distributivity and associativity of the sum operation. 
This is however not the case for the KL divergence.

Similarly to the sum of the item values (in the Knapsack problem), the KL divergence is evaluated on the entire set, but unlike the sum, the items themselves do not have any “value” - and their contribution to the KL divergence changes depending on the other items in the set. 
This leaves us with a limited choice of algorithms to solve the problem. 
Because of the complexity of the problem and its smallish role in this work, we use a simple random search. 

This approach works in two steps:

%% todo pseudocode algorithm for the optimization
\begin{enumerate}
    \item Repeatedly sample a random subset of size $k$ of the search queries, keeping track of the subset with the lowest KL divergence.
After a fixed number of iterations, take the subset with the lowest KL divergence.
    \item From the $k$-sized subset, repeatedly remove the search query with the highest contribution to the divergence.
Stop when the KL-divergence stops decreasing, i.e. we cannot remove any more search queries to decrease the divergence, or we've reached the minimum subset size $l$.
\end{enumerate}

This is a simple and computationally cheap approach, but it might not always find the optimal solution.
Furthermore, since the KL-divergence is a non-convex function, the second step of the algorithm might get stuck in a suboptimal solution.
While this could be mitigated using a more complex optimization algorithm (e.g. simulated annealing etc.), experimental results show that the simple random search is sufficient for our purposes.

\begin{figure}[ht!]
    \captionsetup{width=.9\linewidth}
    \includegraphics[width=0.8\textwidth]{../img/all-queries-vs-totals-corrected.png}
    \centering
    \caption{Comparing the faculty distribution of the search query results \textit{(orange, right)} to the distribution of all the publications in the system \textit{(blue, left)} \textit{after running the optimization algorithm}.}
\end{figure}

Running the optimization algorithm with $n = 10000$ initial random samples of size $k = 300$ on the search query set, we arrive at a set of $174$ queries with the search result KL-divergence of \texttt{0.00317}. 
This is a significant improvement over the original divergence of \texttt{0.0471} and we will consider this a fairer subset of the search queries for the benchmarking.

\subsection[Collecting the data]{Collecting the data\textsuperscript{\href{https://barjin.github.io/edu/thesis-blog/collecting-data/}{[blog]}}}\label{collecting-data}

Since our proposed benchmark only evaluates the search results ranking, we collect the search results for the benchmark queries in advance.
Similarly, we also collect the data from the Scopus API for the same queries, as the automated relevance feedback.

While collecting the search results from the Charles Explorer search engine is straightforward since the API is available to us, 
the Scopus Advanced search feature requires us to use a special query language\footnote{\url{https://schema.elsevier.com/dtds/document/bkapi/search/SCOPUSSearchTips.htm}} to submit the search queries. 
This query language offers a set of Prolog-like functors, 
each related to a specific field - or a set of fields - of the publication record. 
The attributes of these functors are used in a \textit{substring search} on the specified fields.

Apart from this, the query language also supports logical operators, such as \texttt{AND}, \texttt{OR}, and \texttt{AND NOT}.

We will use two of the available functors: \texttt{TITLE-ABS-KEY} and \texttt{AF-ID}:

\begin{itemize}
    \item \texttt{TITLE-ABS-KEY} searches for the specified substring in the title, abstract, and keywords of the publication record. 
    In this regard, it is similar to the full-text search in Charles Explorer, which searches in the same fields.
    \item \texttt{AF-ID} filters the search results by the organisation affiliation of the publication. This is useful for filtering the search results to only those publications where at least one of the authors is affiliated with Charles University. Since Elsevier Scopus contains many records not affiliated with Charles University (but Charles Explorer only contains such records), this will help us to get a more comparable sets of search results.
\end{itemize}

By calling the Scopus API, we get the search results in JSON format, which we then process and store in our database for the benchmarking.

\begin{figure}[ht!]
        \scriptsize
        \centering
        \begin{tabular}{|l|l|l|l|l|l|l|}
        \hline
            ~ & ranking & totalAuthors & scopusId & pubYear & citationCount & referenceCount \\ \hline
            ranking & 1.000000 & 0.038005 & 0.081229 & 0.109848 & 0.062467 & 0.053487 \\ \hline
            totalAuthors & 0.038005 & 1.000000 & 0.033948 & 0.040538 & 0.113336 & 0.094358 \\ \hline
            scopusId & 0.081229 & 0.033948 & 1.000000 & 0.806411 & 0.015393 & 0.243830 \\ \hline
            pubYear & 0.109848 & 0.040538 & 0.806411 & 1.000000 & 0.033019 & 0.283521 \\ \hline
            citationCount & 0.062467 & 0.113336 & 0.015393 & 0.033019 & 1.000000 & 0.218415 \\ \hline
            referenceCount & 0.053487 & 0.094358 & 0.243830 & 0.283521 & 0.218415 & 1.000000 \\ \hline
        \end{tabular}
    \caption{Correlation matrix of the Elsevier Scopus search results numeric attributes.} 
\end{figure}

We see that the \texttt{ranking} column - the position of a publication in the search results - is only very weakly correlated with the other numeric attributes of the search results.
This suggests that the default Scopus ranking is mostly influenced by the full-text search relevance and does not take any further attributes into account.
The strongest correlated attribute is the \texttt{pubYear} - this suggest that the older publications are ranked higher in the search results.
However, the absolute value of the correlation coeficient (\texttt{0.109848}) is still very low.

It also suggest that the Scopus result ranking might not be too dependent on the social network measures and that we might not be able to improve the ranking 
by using the social network data (as the ``explicit'' social network data like the citation count or the reference count are already not correlated with the ranking).

\subsection[Simulating relevance feedback]{Simulating relevance feedback\textsuperscript{\href{https://jindrich.bar/edu/thesis-blog/ndcg-benchmark/}{[blog]}}}

With the data collected, we now proceed with the actual analysis of the search results ranking in Charles Explorer.

Considering the Scopus search results as the gold standard, we calculate the per-query precision, recall and $F_1$ score for the search results of Charles Explorer.

\begin{figure}[!ht]
    \captionsetup{width=.9\linewidth}
    \centering
    \begin{tabular}{|l|l|l|l|}
    \hline
        \textbf{Query} & \textbf{Precision} & \textbf{Recall} & \textbf{$F_1$ score} \\ \hline
        physics & 0.043011 & 0.040000 & 0.041451 \\ \hline
        bolus & 0.125000 & 0.121212 & 0.123077 \\ \hline
        draft & 0.010870 & 0.010753 & 0.010811 \\ \hline
        $\hdots$ & $\hdots$ & $\hdots$ & $\hdots$ \\ \hline
    \end{tabular}
    \caption{Per-query precision, recall and $F_1$ score for the search results of Charles Explorer.}
\end{figure}

After aggregation over all the queries, this gives us the following unfavourable statistics:

\begin{figure}[!ht]
    \captionsetup{width=.9\linewidth}
    \centering
    \begin{tabular}{|l|l|}
    \hline
        Mean & 0.208727 \\ \hline
        Standard deviation & 0.211699 \\ \hline
        Minimum & 0.010101 \\ \hline
        25\% & 0.074786 \\ \hline
        50\% & 0.137028 \\ \hline
        75\% & 0.265263 \\ \hline
        Maximum & 1.000000 \\ \hline
    \end{tabular}
    \caption{Aggregated statistics of the $F_1$ score for the search results of Charles Explorer.}
\end{figure}

We see that the current Charles Explorer search results differ quite a lot from the Scopus search results. 
This can be caused by mutiple reasons - either the publications are not present in the Scopus database, or the queries are not specific enough and the search results are returning partially disjoint sets of publications.

Note that this is an issue that goes beyond re-ranking the Charles Explorer search results. 
We cannot quantify the benefit of reordering the search results if we consider all the search results irrelevant.
This hinders our ability to use the Scopus search results ranking as the proxy for the relevance feedback.

Since this thesis is focused on the search result ranking algorithms, we will proceed with the benchmarking as planned. 
However, to improve the relevance score assignment, we add a \textit{similarity search} step. 
 
In the $F_1$ score calculation, we are currently only matching the Charles Explorer search results with the Scopus search results by the \textit{publication title} (case-insensitive). 
This matching criterion is prone to even the slightest variations in the publication titles, which can lead to false negatives.

\subsubsection[Inferring the publication relevance with semantic search]{Inferring the publication relevance with semantic search\textsuperscript{\href{https://jindrich.bar/edu/thesis-blog/ndcg-benchmark/\#infering-the-relevance-with-similarity-search}{[blog]}}}\label{semantic-search}

In the proposed \textit{similarity search} step, we use the similarity of LLM (Large Language Model) embeddings to match the publication titles. 
This should help us to relate the publications missing from the Scopus search results to the ones present there and assign them a relevance score.

\begin{mybox}{LLM embeddings}

    \textit{LLM embeddings} are vector representations of a given text, generated by a large language model.
    While those can be arbitrary vectors, embeddings are usually optimized to capture the semantic meaning of the text. 
    
    This means that texts with similar meanings should have similar embeddings - i.e. the (cosine) similarity of the embedding vectors should be high.
\end{mybox}

We enhance the relevance calculation with the similarity search process as follows:

\begin{enumerate}
    \item By the means of an \textit{LLM embedding model}, we precalculate the embeddings for the publication titles of the Elsevier Scopus search results. 
    We store these embeddings in a vector database.
    \item For each publication title in the Charles Explorer search results, we calculate its embedding. 
    In the database, we search for the nearest embedding among Scopus search results embeddings. 
    Futhermore, we require the retrieved document to be a result of the same query (in Elsevier Scopus) as the Charles Explorer search result.
    \item We calculate Charles Explorer document’s inferred relevance from the most similar document’s attributes - e.g. its position in Scopus search.
\end{enumerate}

For the document embedding, we use the \texttt{all-MiniLM-L6-v2}\footnote{\url{https://www.sbert.net/docs/sentence_transformer/pretrained_models.html}} sentence - transformer model. 
This is a general-purpose English embedding language model designed for running on consumer-grade hardware. 
Due to its small size and competitive performance, it’s often used for the real-time use-cases, like semantic search or RAG (Retrieval-Augmented Generation).

For the similarity search on the embeddings we use the ChromaDB database\footnote{\url{https://www.trychroma.com/}}. 
ChromaDB is a vector database designed for the similarity search on the embeddings, with support
for enhancing the search results with the additional metadata attributes of the documents.

\begin{figure}[ht!]
    \captionsetup{width=.9\linewidth}
    \includegraphics[width=0.8\textwidth]{../img/llm-embedding-positions-hist.png}
    \centering
    \caption{Histogram of inferred positions of the Charles Explorer search results in the Scopus search results.}
\end{figure}

This process gives us the predicted rankings which follow a rather skewed distribution. 
However, this does not pose a serious problem to our benchmark.

Firstly, we are not trying to predict the exact ranking of the search results, but rather to assign a relevance score to each search result. 
The peak of the distribution is at the top of the rankings, 
which is in line with the well known tendency of human users 
to have much clearer opinion about the few top results than the long tail of the search results (as described by e.g. \cite{9357332}).

Secondly, the left-skewed distribution might be caused by non-uniform lengths of the search result lists. 
Since for some of the queries, Scopus returns only a few relevant search results (100 is only the maximum limit), 
the resulting predicted rankings will be skewed towards the top of the list for these queries.

\section{Evaluation}

Using the original ranking positions and the predicted ranking positions as the source for the 
relevance feedback, we calculate the \textit{nDCG} (Normalized Discounted Cumulative Gain) score for the 
search result ranking.

DCG score for a single search result list is calculated as the sum of the relevance scores of the search results, discounted by their position in the ranking.

$$
DCG_\text{list} = \sum_{i=1}^{N} \frac{2^{rel_i} - 1}{\log_2(i + 1)}
$$

The IDCG score is the DCG score of the ideal ranking of the search results, i.e. the items in the list are sorted in descending order by $rel_i$.

Normalized DCG score is then calculated as the ratio of the DCG score to the IDCG score.

$$
nDCG_{\text{list}} = \frac{DCG_\text{list}}{IDCG_\text{list}}
$$

To transform the predicted Scopus rankings from \ref{semantic-search} into relevance feedback, 
we introduce a new function $rel_q(d)$.

For a given query $q$, the document $d$ is considered to have relevance of $rel_q(d)$, 
which is \textit{inversely proportional} to its predicted ranking. 
This is necessary for the \textit{nDCG} score calculation, which requires more relevant documents 
to have higher relevance scores.

The inverse proportionality is achieved by the following formula:
\[rel_q(d) = \frac{a}{\text{rank}_q(d) + 1}\]

where $a$ is a constant that scales the relevance scores and can help achieve better 
stability of the nDCG score with respect to rounding errors.

For the purpose of the experiments in this thesis, we set $a = 5$. 
The $+1$ in the denominator is necessary to avoid division by zero, as our rankings are $0-$based.

While it would be possible to achieve the ranking $\rightarrow$ relevance transformation via e.g. subtracting the predicted ranking from the total number of search results, 
our proposed method with $rel_q(d)$ introduces a \textit{non-linear} transformation of the predicted rankings. 
This differentiates better between the search results that are ranked higher in the Scopus search results. 
This is again in line with the beforementioned tendency of the human users to have clearer opinions about the top search results.

\subsection{Baseline benchmark}\label{baseline-benchmark}

Calculating \texttt{nDCG} over search results for individual queries in
Charles Explorer with the predicted relevance scores from \ref{semantic-search} gives us the following results:

\begin{figure}[!ht]
    \captionsetup{width=.9\linewidth}
    \centering
    \begin{tabular}{|l|l|l|l|}
    \hline
        ~ & dcg & idcg & ndcg \\ \hline
        mean & 14.919819 & 19.167405 & 0.761607 \\ \hline
        std & 16.810894 & 17.665142 & 0.180979 \\ \hline
        min & 0.094340 & 0.094340 & 0.405669 \\ \hline
        25\% & 5.250473 & 7.704989 & 0.627563 \\ \hline
        50\% & 9.527864 & 14.840570 & 0.736246 \\ \hline
        75\% & 18.064385 & 24.112511 & 0.934206 \\ \hline
        max & 104.693354 & 104.693354 & 1.000000 \\ \hline
    \end{tabular}
    \caption{Aggregated statistics of the nDCG score for the original search results of Charles Explorer (\textit{query count = $149$}).}
\end{figure}

The mean nDCG score of $0.761607$ suggests that the search result ranking in Charles Explorer already works well - and that the relevance feedback based on the predicted Scopus rankings gives us a good approximation of the relevance of the search results.

\subsection{Using graph metrics for re-ranking}

With the relevance feedback and baseline benchmark values established, we can now proceed with the re-ranking of the search results in Charles Explorer using the social network data.
After retrieving the metrics listed in \hyperref[social-network-metrics]{Subsection \ref*{social-network-metrics}},
we train two models for aggregating the social network metrics and the original relevance scores to predict the final relevance scores.

As mentioned in \hyperref[combining-metrics]{Subsection \ref*{combining-metrics}}, we use two different approaches for the aggregation.

\textbf{Linear combination}: The first proposed solution suggests a linear combination of the original relevance scores and the auxiliary relevance scores.
To find the best coefficients of the linear combination, we use a simple least squares regression model.

By training a regularized linear regression model on the training data ($n=9600$) (i.e. original relevance-based ranking and the social network metrics), we find the coefficients that minimize the \ac{MSE} of the predicted relevance scores.

Evaluation of this model on the test data ($n = 2265$) gives us a \ac{MSE} of $2.1016$.
While this is not interpretable as is - since we are learning an arbitrary relevance score, not a ranking - we can use the predicted relevance scores to rerank the search results and calculate the nDCG score.

Furthermore, the simplicity of the linear regression model allows us to inspect the internal coefficients learned by the model and
to interpret the importance of the individual features for the final relevance score.

\begin{figure}[!ht]
    \captionsetup{width=.9\linewidth}
    \centering
    \begin{tabular}{|l|l|}
    \hline
        \textbf{Feature} & \textbf{Coefficient} \\ \hline
        charles\_explorer\_ranking & -0.23068 \\ \hline
        centrality\_1 & 0.08317 \\ \hline
        centrality\_2 & 0.01018 \\ \hline
        degree & -0.08756 \\ \hline
        katz\_centrality & -0.00491 \\ \hline
        node\_cut & -0.02733 \\ \hline
    \end{tabular}
    \caption{Coefficients of the linear regression model for the linear combination of the original relevance scores and the social network metrics.}
\end{figure}

From the coefficients, we can see that the most important feature for the final relevance score is the original relevance score - i.e. the ranking of the search results in Charles Explorer.
This seemingly confirms our hypothesis that the Scopus search result ranking is mainly influenced by the relevance of the search results themselves, rather than any other - more global - publication measures.

\textbf{Neural networks}: The second proposed solution suggests training a neural network to predict the final relevance scores based on the original relevance scores and the social network metrics.
As mentioned before, this might help with finding the non-linear relationships between the features and the final relevance scores.
On the other side, such model is less interpretable than the linear regression model.

Training an neural network ($2$ hidden layers, $100$ neurons each) on the same training dataset ($n=9600$) as the linear regression model gives us a \ac{MSE} of $2.0498$ on the test data ($n=2265$).
We see that in this regard, the neural network model outperforms the linear regression model only marginally.
This further suggests that the relationships between the features and the final relevance scores are rather simple and can be captured by a linear model.

Using both models to aggregate the new ``predicted'' relevance scores, we calculate the nDCG score for the re-ranked search results.
We compare the NDCG scores of the re-ranked search results to the baseline benchmark to determine the performance of the social network-enhanced search engine.

\begin{figure}[!ht]
    \centering
    \begin{tabular}{|l|l|l|l|}
    \hline
        ~ & \textbf{Baseline nDCG} & \textbf{Linear model} & \textbf{Neural network} \\ \hline
        count & 149.000000 & 149.000000 & 149.000000 \\ \hline
        mean & 0.761607 & 0.746176 & 0.770519 \\ \hline
        std & 0.180979 & 0.172590 & 0.178775 \\ \hline
        min & 0.405669 & 0.404108 & 0.425830 \\ \hline
        25\% & 0.627563 & 0.618646 & 0.615640 \\ \hline
        50\% & 0.736246 & 0.735163 & 0.759933 \\ \hline
        75\% & 0.934206 & 0.904977 & 0.955842 \\ \hline
        max & 1.000000 & 1.000000 & 1.000000 \\ \hline
    \end{tabular}
    \captionsetup{width=.9\linewidth}
    \caption{Comparison of the nDCG scores of the baseline search results and the search results re-ranked using the linear regression and neural network models.}
\end{figure}

We see that the nDCG score for both the models utilizing the graph measures is lower - or comparable - 
to the nDCG score of the original search results.

This shows that the graph measures we have collected do not provide much useful information 
for the reranking of the search results. 

Note that while the evaluation of the reranking performance is done on the entire set 
(including the linear regression / neural network training set), 
the results are still not any better than the original search results. 
This hints at the high dimensionality of the problem and the lack of generalization of either of the models.

As mentioned before, the Scopus search result ranking is likely mainly influenced by the relevance of 
the search results themselves, rather than any other - more global - publication measures. 
In a way, the fact that the graph measures do not help with the reranking does not come as a surprise. 
This might be also partially caused by the title similarity search step, which helps with the missing publications problem.

This shows that the Scopus search result ranking is likely not a good proxy for the relevance feedback,
if searching for a global relevance measure of the publications.

\section[Predicting the citation count]{Predicting the citation count\textsuperscript{\href{https://jindrich.bar/edu/thesis-blog/citation-count/}{[blog]}}}

As we have noticed before, the search results ranking we have acquired from Scopus is mostly based on the raw query relevance. 
Because of this, it might be also riddled with the problems we have discussed in \hyperref[search-ranking-issues]{Subsection \ref*{search-ranking-issues}} - 
such as the susceptibility to (potentially malicious) search engine optimization.

In search for a better ranking system for Charles Explorer, we try to predict the \textit{citation count} of the papers based on the local graph measures we have calculated in the previous section. 
This decision comes from the idea that the \textit{citation count} is a good indicator of the importance of the paper that cannot be easily interfered with by the authors.

Unlike the search result ranking optimization from the previous sections, the citation count inference is a popular research topic in the field of bibliometrics.
Notably, \cite{ABBASI2011594} explores the influence of small scale graph neighborhoods of authors on their academic performance and the citation count of their papers.

\subsection{Sourcing the data}

Unfortunately, in the data collected for the previous sections' benchmarking we do not have the citation count of the publications.
Reusing the Scopus API access from \hyperref[collecting-data]{Subsection \ref*{collecting-data}}, we can retrieve the citation count for the publications in our system.

This shrinks the dataset to $n = 962$ publications, as the Scopus API does not contain all the publications in our system.
While this is a significant reduction in the dataset size, we still consider this representative and large enough for the purposes of this thesis.

\subsection{Training the models}

As in the previous section, we train two models for predicting the citation count of the publications based on the local graph measures.

\textbf{Linear regression}: The first model is a simple linear regression model, which tries to predict the citation count of the publications based on the local graph measures.
The model is trained on the training dataset ($n=768$) and evaluated on the test dataset ($n=194$). 
On the test dataset, the model achieves a \ac{MSE} of $6725.05077$.

As in the previous experiment, the model allows us to inspect the coefficients of the individual features and interpret their importance for the prediction of the citation count.

\begin{figure}[!ht]
    \captionsetup{width=.9\linewidth}
    \centering
    \begin{tabular}{|l|l|}
    \hline
        \textbf{Feature} & \textbf{Coefficient} \\ \hline
        centrality\_1 & 13.60649 \\ \hline
        centrality\_2 & -12.04390 \\ \hline
        degree & 35.42298 \\ \hline
        katz\_centrality & -5.46456 \\ \hline
        node\_cut & 3.06015 \\ \hline
    \end{tabular}
    \caption{Coefficients of the linear regression model for predicting the publication citation count.}
\end{figure}

Note that from the linear regression coefficients, it seems that the \texttt{degree} of the publication is the most important feature for the prediction of the citation count (all the features are normalized to the same scale before training the regression).
This is in line with the findings of \cite{ABBASI2011594}, which - while inspecting slightly different graph measures - also found the \texttt{degree} of the publications to be the most important feature for the prediction of the citation count.

\textbf{Neural network}: The second model is a neural network model, which tries to predict the citation count of the publications based on the local graph measures.
Training an neural network ($2$ hidden layers, $100$ neurons each) on the training dataset ($n=768$) gives us a \ac{MSE} of $7425.57273$ on the test data ($n=194$).

Note that in terms of the \ac{MSE}, neither of the models is performing especially well - in the absolute numbers of \ac{RMSE}, the models are off by approximately $80$ citations on average.
This is a significant error, considering that the average citation count in the dataset is $30$.

\subsection[Citation-based ranking]{Citation-based ranking\textsuperscript{\href{https://jindrich.bar/edu/thesis-blog/citation-count/\#citation-based-ranking}{[blog]}}}\label{citation-based-ranking}

Note that despite the unfavourable results of the previous experiments - i.e. predicting the absolute value of the citation count - we might still be able to use the citation count as a proxy for the global relevance of the publications.

This is beause the ranking benchmarks (and human users of search engines) are usually not be interested in the exact citation count, but rather in the relative importance of the publications. 
Both benchmarks and users also tend to discount the publications further down in the search result list.

Because of our original goal, i.e. reorder the search results to set the more “globally” important papers higher,
we might try to use the predicted citation count as the relevance score for the search results ranking.

In the last experiment, we calculate the nDCG score of the search results in Charles Explorer with the true citation count as the relevance feedback and the predicted citation count based on the graph metrics as the ordering.

\begin{figure}[ht!]
    \captionsetup{width=.9\linewidth}
    \includegraphics[width=0.8\textwidth]{../img/ndcg_citations.png}
    \centering
    \caption{nDCG scores of $100$ queries with different citation count prediction methods.}
\end{figure}

We see that both the methods utilizing the graph measures for the citation count prediction are performing much better than the baseline benchmark.
This confirms our hypothesis from the beginning of this subsection - while the graph measures might not provide enough data for the absolute prediction of the citation count, 
they can still be well utilized for the relative ranking of the publications.