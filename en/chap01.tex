\chapter{Definitions and notation}

The initial chapter lays out the definitions of the most essential social network-related terms and concepts. 
The chapter also introduces the notation used throughout the thesis.

\section{Discrete graphs}

In discrete mathematics, \textit{graph} is a mathematical structure that consists of a set of \textit{nodes} (often denoted as $V$ for \textit{vertices})
and a set of \textit{edges} (often denoted as $E$) that connect pairs of the nodes.

This section introduces some of the thesis's less common graph-related terms and concepts.

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\begin{definition}[Adjacency matrix]
    The \textit{adjacency matrix} of a graph $G = (V, E)$ is a square matrix $A$ of size $|V| \times |V|$ where $A_{uv} = 1$ if there is an edge between nodes $u$ and $v$, and $A_{uv} = 0$ otherwise.  

    Different matrix operations can be used to calculate various properties of the graph. For example, the degree of a node can be calculated as the sum of the elements in the row of the adjacency matrix corresponding to the node.
\end{definition}

\begin{definition}[Distance matrix]
    The \textit{distance matrix} of a graph $G = (V, E)$ is a square matrix $D$ of size $|V| \times |V|$ where $D_{uv}$ is the length of the shortest path between nodes $u$ and $v$.

    The distance matrix can be calculated using Floyd-Warshall algorithm or repeated Dijkstra's algorithm.
\end{definition}

\begin{definition}[Node neighborhood]
    The \textit{neighborhood} of a node $v$ in a graph $G$ is the set of all nodes that are connected to $v$ by an edge.
    $$
    N(v) = \{ u \in V \mid \text{there exists an edge between $v$ and $u$} \}
    $$
\end{definition}

\begin{definition}[k-hop neighborhood]
    The \textit{k-hop neighborhood} of a node $v$ in a graph $G$ is the set of all nodes that are reachable from $v$ by traversing at most $k$ edges.

    $$
    N_k(v) = \{ u \in V \mid \text{there exists a path of length at most $k$ from $v$ to $u$} \}
    $$

    This can be useful to capture the local structure of a graph 
    - e.g., \cite{nikolentzos2019khop} use the concept of k-hop neighborhoods to enhance graph embeddings in \ac{GNN}.
\end{definition}

\begin{definition}[Subgraph]
    A \textit{subgraph} $G' = (V', E')$ of a graph $G = (V, E)$ is a graph where $V' \subseteq V$ and $E' \subseteq E$.
\end{definition}

\begin{definition}[Induced subgraph]
    An \textit{induced subgraph} $G' = (V', E')$ of a graph $G = (V, E)$ is a subgraph where $V' \subseteq V$ and $E' = \{ (u, v) \in E \mid u, v \in V' \}$.

    In simpler terms, an \textit{induced subgraph} is a subgraph that contains all the edges between the nodes in the subgraph.
\end{definition}

\begin{definition}[Bipartite graph]
    A \textit{bipartite graph} is a graph where the nodes can be divided into two disjoint sets $V_1$ and $V_2$ such that all edges connect nodes from $V_1$ to nodes from $V_2$.
    
    Formally, a graph $G = (V, E)$ is bipartite if there exists a partition of $V$ into two sets $V_1$ and $V_2$ such that
    $$E \subseteq \{ (u, v) \mid u \in V_1, v \in V_2 \}$$
\end{definition}

\begin{definition}[Monopartite projection]\label{def:monopartite-projection}
    The \textit{monopartite projection} of a bipartite graph $G = (V_1 \cup V_2, E)$ onto a set of nodes $V_1$ is a graph $G' = (V_1, E')$, 
    where the nodes are the nodes in $V_1$, and the edges are between the nodes in $V_1$ that share a common neighbor in $V_2$.
    
    Formally defined, the monopartite projection is a graph $G' = (V_1, E')$ where
    $$
    E' = \{ (u, v) \mid \exists w \in V_2 \text{ such that } (u, w) \in E \text{ and } (v, w) \in E \}
    $$

    Projection algorithms can produce multiple edges between the nodes in the projection.
    The resolution of this problem largely depends on the specific use case of the projection â€” e.g., $G'$ can be a multigraph, or the edges can be weighted by the number of common neighbors from $V_2$.
    
\begin{definition}[Connected component]
    A \textit{connected component} of a graph $G = (V, E)$ is a subgraph $G' = (V', E')$ where all nodes in $V'$ are reachable from each other by traversing the edges in $E'$.
\end{definition}

\end{definition}
\section{Social networks}

While the social sciences have been studying social networks for decades, not all of the terminology is relevant to this thesis.
This section introduces the most essential computation-related social network terms and concepts used in the thesis.

For this thesis, a \textit{social network} is a graph where the nodes represent real-world entities (people, their publications) and the edges represent relationships between the entities (only authorship in this thesis).

\begin{definition}[Ego network]
    The \textit{ego network} of a node $v$ in a social network $G$ is the induced subgraph of $G$ that contains $v$ and its (1-hop) neighborhood.
    $$
    E(v) = (N(v), \{ (v, u) \mid u \in N(v) \})
    $$
\end{definition}

\begin{definition}[Ego (vertex)]
    The \textit{ego} of a node $v$ in an ego network $E(v)$ is the node $v$ itself.
\end{definition}

\begin{definition}[Alter (vertex)]
    An \textit{alter} of a node $v$ in an ego network $E(v)$ is any node $u$ that is not the ego $v$.
\end{definition}

\section{Classifier evaluation metrics}

In the context of this thesis, the social network metrics are used to evaluate 
the performance of the classifiers that are run on the social network data.

This section introduces the most common classifier evaluation metrics used in the thesis.

\begin{definition}[Precision]
    The \textit{precision} of a classifier is the ratio of the true positive predictions to the total number of positive predictions.
    $$
    \text{precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
    $$
\end{definition}

\begin{definition}[Recall]
    The \textit{recall} of a classifier is the ratio of the true positive predictions to the total number of actual positive instances.
    $$
    \text{recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
    $$
\end{definition}

\begin{definition}[F1 score]
    The \textit{F1 score} of a classifier is the harmonic mean of the precision and recall.
    $$
    \text{F1} = 2 \cdot \frac{\text{precision} \cdot \text{recall}}{\text{precision} + \text{recall}}
    $$
\end{definition}

\begin{definition}[Macro-averaged F1 score]
    For a multi-class classification task, the \textit{macro-averaged F1 score} is the average of the F1 scores for each class.
    $$
    \text{macro-F1} = \frac{1}{n} \sum_{i=1}^{n} \text{F1}_i
    $$
\end{definition}

\begin{definition}[Micro-averaged F1 score]
    For multi-class classification tasks, the \textit{micro-averaged F1 score} is the F1 score calculated from the total number of true positives, false positives, and false negatives.
    $$
    \text{micro-F1} = 2 \cdot \frac{\sum_{i=1}^{n} \text{TP}_i}{\sum_{i=1}^{n} \text{TP}_i + \sum_{i=1}^{n} \text{FP}_i + \sum_{i=1}^{n} \text{FN}_i}
    $$

    This metric is less susceptible to class imbalance than the macro-averaged F1 score.
    Each class contributes to the micro-averaged F1 score proportionally to the number of instances in the class.
\end{definition}